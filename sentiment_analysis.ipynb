{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7eb76268",
   "metadata": {},
   "source": [
    "# [Practice] Sentiment Analysis Practice \n",
    "- 한국 식당 리뷰 감성분석\n",
    "- 커널 데이터 전처리 부분은 스킵하고 실행해주세요"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7eaea23",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacf2797",
   "metadata": {},
   "source": [
    "### 데이터 로드"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb42465",
   "metadata": {},
   "source": [
    "데이터 출처: https://huggingface.co/datasets/leey4n/KR3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a98bbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "kr3 = load_dataset(\"leey4n/KR3\", split='train')\n",
    "kr3 = kr3.remove_columns(['__index_level_0__']) # Original file didn't include this column. Suspect it's a hugging face issue.\n",
    "\n",
    "# 레이블이 2(모호한 답변)인 리뷰 drop\n",
    "kr3_binary = kr3.filter(lambda example: example['Rating'] != 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c441ce53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rating</th>\n",
       "      <th>Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>숙성 돼지고기 전문점입니다. 건물 모양 때문에 매장 모양도 좀 특이하지만 쾌적한 편...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>고기가 정말 맛있었어요! 육즙이 가득 있어서 너무 좋았아요 일하시는 분들 너무 친절...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>잡내 없고 깔끔, 담백한 맛의 순댓국이 순댓국을 안 좋아하는 사람들에게도 술술 넘어...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>고기 양이 푸짐해서 특 순대국밥을 시킨 기분이 듭니다 맛도 좋습니다 다만 양념장이 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>순댓국 자체는 제가 먹어본 순대국밥집 중에서 Top5 안에는 들어요. 그러나 밥 양...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459016</th>\n",
       "      <td>0</td>\n",
       "      <td>731 배달 시켜 먹었고요, 거리상 1.8km입니다. 배민에서 시켰고 정확히 58만...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459017</th>\n",
       "      <td>1</td>\n",
       "      <td>송탄 미군부대 근처에 위치한 곳 원래 로컬 맛 집으로 되게 유명했는데 삼대 천왕에 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459018</th>\n",
       "      <td>1</td>\n",
       "      <td>집에서 40킬로 정도 떨어져 있는 곳인데도 몇 달에 한 번은 이거 먹으러 일부러 갑...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459019</th>\n",
       "      <td>0</td>\n",
       "      <td>원래 글 안 쓰는데 이거는 정말 다른 분들 위해서 써야 할 것 같네요 방금 포장 주...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459020</th>\n",
       "      <td>1</td>\n",
       "      <td>우리 팀 단골집, 술 먹고 다음 날 가면 푸짐하게 배불리 해장할 수 있는 곳, 주말...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>459021 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Rating                                             Review\n",
       "0            1  숙성 돼지고기 전문점입니다. 건물 모양 때문에 매장 모양도 좀 특이하지만 쾌적한 편...\n",
       "1            1  고기가 정말 맛있었어요! 육즙이 가득 있어서 너무 좋았아요 일하시는 분들 너무 친절...\n",
       "2            1  잡내 없고 깔끔, 담백한 맛의 순댓국이 순댓국을 안 좋아하는 사람들에게도 술술 넘어...\n",
       "3            1  고기 양이 푸짐해서 특 순대국밥을 시킨 기분이 듭니다 맛도 좋습니다 다만 양념장이 ...\n",
       "4            1  순댓국 자체는 제가 먹어본 순대국밥집 중에서 Top5 안에는 들어요. 그러나 밥 양...\n",
       "...        ...                                                ...\n",
       "459016       0  731 배달 시켜 먹었고요, 거리상 1.8km입니다. 배민에서 시켰고 정확히 58만...\n",
       "459017       1  송탄 미군부대 근처에 위치한 곳 원래 로컬 맛 집으로 되게 유명했는데 삼대 천왕에 ...\n",
       "459018       1  집에서 40킬로 정도 떨어져 있는 곳인데도 몇 달에 한 번은 이거 먹으러 일부러 갑...\n",
       "459019       0  원래 글 안 쓰는데 이거는 정말 다른 분들 위해서 써야 할 것 같네요 방금 포장 주...\n",
       "459020       1  우리 팀 단골집, 술 먹고 다음 날 가면 푸짐하게 배불리 해장할 수 있는 곳, 주말...\n",
       "\n",
       "[459021 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 'reviews' 코퍼스와 'ratings' 라벨 저장\n",
    "reviews = kr3_binary[\"Review\"]\n",
    "ratings = kr3_binary['Rating']\n",
    "\n",
    "# 샘플 확인\n",
    "review_df = pd.DataFrame(kr3_binary, columns=kr3_binary.column_names)\n",
    "display(review_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52dce9e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'잡내 없고 깔끔, 담백한 맛의 순댓국이 순댓국을 안 좋아하는 사람들에게도 술술 넘어갈 듯합니다. 정식 메뉴를 시키면 구성비 좋게 찹쌀순대와 수육이 나오는데 아주 푸짐해요. 해장하러 갔는데 국물이 고소해서 소주를 더 시키게 되는... 여하튼 순댓국이 일품입니다. 송파에선 알아주는 터줏대감인듯해요'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_df['Review'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d49ff567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "긍정 리뷰 비율: 0.8455190503266735 | 부정 리뷰 비율: 0.15448094967332648\n"
     ]
    }
   ],
   "source": [
    "# 긍/부정 비율 확인 (긍정이 압도적으로 많음)\n",
    "positive_rate = len(review_df[review_df['Rating'] == 1]) / len(review_df)\n",
    "negative_rate = 1 - positive_rate\n",
    "print(\"긍정 리뷰 비율:\", positive_rate, \"|\", \"부정 리뷰 비율:\", negative_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1e47a2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a15e95",
   "metadata": {},
   "source": [
    "### 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acf0065c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# 1) 안전 불용어(기능어 중심)\n",
    "safe_stopwords_ko = set(\"\"\"\n",
    "은 는 이 가 을 를 에 의 와 과 도 만 보다 에서 에게 께 부터 까지 마다 한테 처럼 대로 하고 으로 로 으로써 로써\n",
    "그리고 그러나 하지만 또는 그래서 그런데 혹은 따라서 그러므로 고로\n",
    "저 나 너 우리 저희 너희 그 이 저기 거기 여기 무엇 무슨 누구 어디 어느 어떤 이것 그것 저것 이런 그런 저런 이번 이쪽 저쪽 그쪽 이때 그때\n",
    "다 요 죠 네 데 게 지 군 나 니다 합니다 했다 한다 된다 였다\n",
    "때 시간 동안 시각 무렵 이상 이하 전후 이래 이후\n",
    "또 또한 그냥 다시 계속 항상 여전히 아직 가끔 자주 거의\n",
    "아 어 야 응 음 흠 헐 우와 와 오 이야 휴 허 하하 ㅋㅋ ㅎㅎ ㅠㅠ\n",
    "\"\"\".split())\n",
    "\n",
    "# 2) 보호 단어(감성 핵심)\n",
    "protect_sentiment_words = set(\"\"\"\n",
    "별로 좋아 좋다 최고 최악 실망 만족 불만 추천 재방문\n",
    "맛있다 맛없다 싱겁다 짜다 달다 매콤 맵다 느끼하다 담백하다 신선 싱싱 비린 눅눅 바삭 촉촉 부드럽다 질기다\n",
    "양 많다 적다 푸짐 부족 비싸다 싸다 가성비 가심비\n",
    "빠르다 느리다 한산 복잡 붐빈다 대기 기다리다\n",
    "깨끗 청결 더럽 위생 지저분 냄새 쾌적 시끄럽다 조용하다\n",
    "서비스 응대 친절 불친절 상냥 무례 거칠다\n",
    "정말 진짜 너무 매우 아주 꽤 완전\n",
    "안 못 없다 없음 아니라 비추 다시는 다신\n",
    "\"\"\".split())\n",
    "\n",
    "# 3) 1글자 화이트리스트(도메인 핵심)\n",
    "whitelist_1char = set(list(\"맛밥국탕면롤육쌈김술밥빵밥밥양밥밥\")) | {\"맛\",\"밥\",\"국\",\"양\",\"술\",\"김\"}\n",
    "\n",
    "neg_prefix = {\"안\",\"못\",\"안함\",\"않\",\"못함\",\"무\",\"미\",\"불\",\"무척\"}  # 필요시 조정\n",
    "emphasis_words = {\"정말\",\"진짜\",\"너무\",\"매우\",\"아주\",\"완전\",\"꽤\"}\n",
    "\n",
    "def normalize_token(t: str) -> str:\n",
    "    # 숫자 통일 (선택)\n",
    "    if re.fullmatch(r\"\\d+\", t):\n",
    "        return \"NUM\"\n",
    "    # ㅋㅋㅋ, ㅎㅎㅎ 축약 (선택)\n",
    "    t = re.sub(r\"(ㅋ)\\1+\", r\"\\1\\1\", t)\n",
    "    t = re.sub(r\"(ㅎ)\\1+\", r\"\\1\\1\", t)\n",
    "    return t\n",
    "\n",
    "def join_negation(tokens):\n",
    "    \"\"\"'안/못/없다' + (다음 감성 단어) -> 결합 토큰 생성\"\"\"\n",
    "    joined = []\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        t = tokens[i]\n",
    "        if t in {\"안\",\"못\",\"없다\",\"없음\",\"아니다\",\"별로\"} and i+1 < len(tokens):\n",
    "            nxt = tokens[i+1]\n",
    "            # 결합 토큰 만들기\n",
    "            joined.append(f\"{t}_{nxt}\")\n",
    "            i += 2\n",
    "            continue\n",
    "        joined.append(t)\n",
    "        i += 1\n",
    "    return joined\n",
    "\n",
    "def filter_tokens(tokens,\n",
    "                  stopwords=safe_stopwords_ko,\n",
    "                  protect=protect_sentiment_words,\n",
    "                  min_len=1):\n",
    "    out = []\n",
    "    tokens = [normalize_token(t) for t in tokens]\n",
    "    tokens = join_negation(tokens)\n",
    "\n",
    "    for t in tokens:\n",
    "        if t in protect or t in emphasis_words:\n",
    "            out.append(t)                         # 보호\n",
    "            continue\n",
    "        if len(t) < min_len and t not in whitelist_1char:\n",
    "            continue                              # 1글자 필터 (화이트리스트 제외)\n",
    "        if t in stopwords:\n",
    "            continue                              # 불용어 제거\n",
    "        out.append(t)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50197f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 459021/459021 [2:22:51<00:00, 53.55it/s]   \n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from tqdm import tqdm\n",
    "\n",
    "okt = Okt()\n",
    "\n",
    "preprocessed_sentences = []\n",
    "\n",
    "for review in tqdm(reviews):\n",
    "    review = re.sub(r\"[^가-힣0-9\\s]\", \"\", review)   # 구두점, 특수문자, 영문 제거\n",
    "    review = re.sub(r\"\\s+\", \" \", review).strip()   # 여러 공백을 하나의 공백으로\n",
    "    tokens = okt.morphs(review, stem=True, norm=True) # 토큰화\n",
    "    tokens = filter_tokens(tokens)\n",
    "    preprocessed_sentences.append(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52345ff",
   "metadata": {},
   "source": [
    "preprocessed_sentences 저장 & 로드 \n",
    "- 커널 재시작시 시간 줄이기용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53d5a1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, gzip\n",
    "\n",
    "# 저장 (리뷰마다 한 줄)\n",
    "with gzip.open(\"preprocessed_sentences.jsonl.gz\", \"wt\", encoding=\"utf-8\") as f:\n",
    "    for toks in preprocessed_sentences:\n",
    "        json.dump({\"tokens\": toks}, f, ensure_ascii=False)\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15efa1c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27393262",
   "metadata": {},
   "source": [
    "토큰화 및 전처리 완료된 corpus 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "025447b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['숙성', '돼지고기', '전문점', '이다', '건물', '모양', '때문', '매장', '모양', '좀', '특이하다', '쾌적하다', '편이', '고', '살짝', '레트로', '감성', '분위기', '잡다', '모든', '직원', '분들', '께서', '전부', '가능하다', '멘트', '치다', '고기', '초반', '커팅', '까지는', '굽다', '가격', '저렴하다', '편', '아니다_맛', '준수', '하다', '등심', '덧', '살이', '인상', '깊다', '구이', '별로_일', '줄', '알다', '육', '향', '짙다', '얇다', '며', '뻑뻑', '하다', '않다', '하이라이트', '된장찌개', '진짜', '굿', '이다', '버터', '간장', '밥', '골뱅이', '국수', '등', '나중', '더', '맛보다', '하다', '것', '들', '남기다', '두다'], ['고기', '정말', '맛있다', '육즙', '가득', '있다', '너무', '좋다', '일', '하다', '분들', '너무', '친절하다', '좋다', '가격', '조금', '있다', '그만하다', '맛', '이라고', '생각']]\n"
     ]
    }
   ],
   "source": [
    "import json, gzip\n",
    "\n",
    "# preprocessed_sentences 로드\n",
    "preprocessed_sentences = []\n",
    "with gzip.open(\"preprocessed_sentences.jsonl.gz\", \"rt\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        preprocessed_sentences.append(json.loads(line)[\"tokens\"])\n",
    "\n",
    "print(preprocessed_sentences[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8ce72b",
   "metadata": {},
   "source": [
    "단어사전 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6847dbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import json\n",
    "\n",
    "SPECIALS = [\"<pad>\", \"<unk>\"]  # 0: pad, 1: unk\n",
    "\n",
    "class SimpleVocab:\n",
    "    def __init__(self, itos, unk_token=\"<unk>\"):\n",
    "        self._itos = list(itos)                       # index -> token\n",
    "        self._stoi = {t:i for i,t in enumerate(itos)} # token -> index\n",
    "        self.unk = unk_token\n",
    "        self.unk_id = self._stoi.get(unk_token, 1)\n",
    "\n",
    "    def __len__(self): return len(self._itos)\n",
    "    def lookup_indices(self, tokens): return [self._stoi.get(t, self.unk_id) for t in tokens]\n",
    "    def lookup_token(self, idx): return self._itos[idx]\n",
    "    def get_stoi(self): return self._stoi\n",
    "    def get_itos(self): return self._itos\n",
    "\n",
    "    def save(self, path):\n",
    "        with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump({\"itos\": self._itos, \"unk\": self.unk}, f, ensure_ascii=False)\n",
    "\n",
    "    @staticmethod\n",
    "    def load(path):\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            obj = json.load(f)\n",
    "        return SimpleVocab(obj[\"itos\"], unk_token=obj.get(\"unk\",\"<unk>\"))\n",
    "\n",
    "def build_vocab_simple(tokenized_sentences, min_freq=2, max_size=None, specials=SPECIALS):\n",
    "    \"\"\"\n",
    "    min_freq: 최소 등장 빈도 (희귀어를 <unk>로 처리)\n",
    "    max_size: 특수토큰 포함한 전체 vocab 최대 크기 (None이면 제한 없음)\n",
    "    정렬: 빈도 descending, 동률이면 사전순\n",
    "    \"\"\"\n",
    "    counter = Counter()\n",
    "    for sent in tokenized_sentences:\n",
    "        counter.update(sent)\n",
    "\n",
    "    # 빈도 필터\n",
    "    items = [(w, c) for w, c in counter.items() if c >= min_freq]\n",
    "    # 빈도 내림차순, 동률 사전순\n",
    "    items.sort(key=lambda x: (-x[1], x[0]))\n",
    "\n",
    "    # 특수토큰 먼저, 그 뒤 단어들\n",
    "    words = [w for w, _ in items]\n",
    "    itos = list(specials) + words\n",
    "\n",
    "    # max_size 제한 (특수토큰 포함하여 자르기)\n",
    "    if max_size is not None and len(itos) > max_size:\n",
    "        itos = itos[:max_size]\n",
    "\n",
    "    return SimpleVocab(itos, unk_token=\"<unk>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "168a4f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000 [929, 604, 701, 9, 592, 1177, 160, 244, 1177, 38, 492, 1306, 217, 41, 190, 3565, 1172, 43, 388, 364] ['숙성', '돼지고기', '전문점', '이다', '건물', '모양', '때문', '매장', '모양', '좀']\n"
     ]
    }
   ],
   "source": [
    "vocab = build_vocab_simple(preprocessed_sentences, min_freq=2, max_size=30000)\n",
    "ids = vocab.lookup_indices(preprocessed_sentences[0])\n",
    "print(len(vocab), ids[:20], [vocab.lookup_token(i) for i in ids[:10]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03382f28",
   "metadata": {},
   "source": [
    "vocab 만든거 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd93809e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.save(\"vocab.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59b66d0",
   "metadata": {},
   "source": [
    "vocab 다시 로드해서 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0bb57b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = SimpleVocab.load(\"vocab.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23071341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시퀀싱\n",
    "sequenced_tokens = [vocab.lookup_indices(tokens) for tokens in preprocessed_sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d41e978",
   "metadata": {},
   "source": [
    "패딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d28cbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = list(kr3_binary[\"Rating\"])   # 정답 라벨 (0/1)\n",
    "\n",
    "# 입력-라벨 동기화 (빈 시퀀스 제거)\n",
    "filtered_pairs = [(seq, y) for seq, y in zip(sequenced_tokens, ratings) if len(seq) > 0]\n",
    "\n",
    "sequenced_tokens, labels = map(list, zip(*filtered_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "efcc79e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "455822 455822\n",
      "[929, 604, 701, 9, 592, 1177, 160, 244, 1177, 38, 492, 1306, 217, 41, 190, 3565, 1172, 43, 388, 364, 82, 161, 688, 828, 288, 4788, 697, 39, 2623, 4838, 1580, 183, 26, 231, 211, 2413, 3284, 2, 641, 8094, 711, 422, 476, 298, 4714, 116, 150, 707, 93, 4068, 495, 1055, 2750, 2, 13, 4736, 801, 37, 484, 9, 340, 466, 54, 3508, 367, 173, 783, 29, 269, 2, 10, 14, 637, 306]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(len(sequenced_tokens), len(labels))\n",
    "print(sequenced_tokens[0])  \n",
    "print(labels[0])         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "31fc069f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(ids, max_len, pad_id=0):\n",
    "    return ids[:max_len] + [pad_id] * max(0, max_len - len(ids))\n",
    "\n",
    "max_len = 20  # 메모리를 위한 양 \n",
    "pad_id = vocab.get_stoi()[\"<pad>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4d6826cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 455822/455822 [00:01<00:00, 382018.45it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "padded_sequences = [pad(seq, max_len, pad_id) for seq in tqdm(sequenced_tokens)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "abf520b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[929, 604, 701, 9, 592, 1177, 160, 244, 1177, 38, 492, 1306, 217, 41, 190, 3565, 1172, 43, 388, 364]\n",
      "20\n",
      "pad id: 0\n"
     ]
    }
   ],
   "source": [
    "print(padded_sequences[0])\n",
    "print(len(padded_sequences[0]))     # max_len = 130 확인\n",
    "print(\"pad id:\", pad_id)        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53551634",
   "metadata": {},
   "source": [
    "Torch Tensor 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d371254a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "inputs = torch.tensor(padded_sequences, dtype=torch.long)           # shape: (N, T=130)\n",
    "labels = torch.tensor(labels, dtype=torch.float32).unsqueeze(1)     # shape: [N, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a90004",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e9ba7d",
   "metadata": {},
   "source": [
    "### 모델 생성 및 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8ac324",
   "metadata": {},
   "source": [
    "모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7a2806bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  929,   604,   701,  ...,    43,   388,   364],\n",
       "        [   39,    22,     4,  ...,  4080,     5,   243],\n",
       "        [  388, 17317,   458,  ...,    53,     2,   507],\n",
       "        ...,\n",
       "        [   18,     8,  5092,  ...,   598,     2,    18],\n",
       "        [  341,   745,  2118,  ...,    30,   143,   136],\n",
       "        [  915,   768,    18,  ...,   404,  8189,  1480]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0a8145f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        ...,\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d939a4a",
   "metadata": {},
   "source": [
    "모델 인스턴스 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1a56ee59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "PAD_ID = 0  # your PAD token id\n",
    "\n",
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_classes):\n",
    "        super().__init__()\n",
    "        # keep PAD embedding fixed at zeros\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=PAD_ID)\n",
    "        self.rnn_cell = nn.RNNCell(embed_size, hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, L) LongTensor\n",
    "        B, L = x.size()\n",
    "        device = x.device\n",
    "\n",
    "        # lengths of each sequence (exclude PAD)\n",
    "        lengths = x.ne(PAD_ID).sum(dim=1)            # (B,)\n",
    "\n",
    "        # init hidden\n",
    "        h = torch.zeros(B, self.rnn_cell.hidden_size, device=device)\n",
    "\n",
    "        # only update hidden for timesteps < length for each sample\n",
    "        max_T = int(lengths.max().item()) if L > 0 else 0\n",
    "        for t in range(max_T):\n",
    "            xt = self.embedding(x[:, t])            # (B, E)\n",
    "            h_new = self.rnn_cell(xt, h)            # (B, H)\n",
    "            mask = (t < lengths).unsqueeze(1)       # (B, 1) bool\n",
    "            h = torch.where(mask, h_new, h)         # keep h for padded samples\n",
    "\n",
    "        logits = self.fc(h)                          # (B, 1)\n",
    "        return self.sigmoid(logits)                  # BCELoss-compatible\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362ee8ab",
   "metadata": {},
   "source": [
    "모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "69becf0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNNClassifier(\n",
      "  (embedding): Embedding(30001, 64, padding_idx=0)\n",
      "  (rnn_cell): RNNCell(64, 64)\n",
      "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 모델 생성\n",
    "VOCAB_SIZE = len(vocab) + 1\n",
    "EMBED_SIZE = 64\n",
    "HIDDEN_SIZE = 64\n",
    "NUM_CLASSES = 1\n",
    "\n",
    "model = RNNClassifier(\n",
    "    vocab_size=VOCAB_SIZE, \n",
    "    embed_size=EMBED_SIZE, \n",
    "    hidden_size=HIDDEN_SIZE, \n",
    "    num_classes=NUM_CLASSES\n",
    ")\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2fb8f087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 | Loss: 0.7130\n",
      "Epoch 2/50 | Loss: 0.5841\n",
      "Epoch 3/50 | Loss: 0.4802\n",
      "Epoch 4/50 | Loss: 0.4220\n",
      "Epoch 5/50 | Loss: 0.4530\n",
      "Epoch 6/50 | Loss: 0.4375\n",
      "Epoch 7/50 | Loss: 0.4122\n",
      "Epoch 8/50 | Loss: 0.4152\n",
      "Epoch 9/50 | Loss: 0.4196\n",
      "Epoch 10/50 | Loss: 0.4077\n",
      "Epoch 11/50 | Loss: 0.3959\n",
      "Epoch 12/50 | Loss: 0.3919\n",
      "Epoch 13/50 | Loss: 0.3903\n",
      "Epoch 14/50 | Loss: 0.3844\n",
      "Epoch 15/50 | Loss: 0.3730\n",
      "Epoch 16/50 | Loss: 0.3619\n",
      "Epoch 17/50 | Loss: 0.3572\n",
      "Epoch 18/50 | Loss: 0.3534\n",
      "Epoch 19/50 | Loss: 0.3429\n",
      "Epoch 20/50 | Loss: 0.3280\n",
      "Epoch 21/50 | Loss: 0.3156\n",
      "Epoch 22/50 | Loss: 0.3068\n",
      "Epoch 23/50 | Loss: 0.2982\n",
      "Epoch 24/50 | Loss: 0.2885\n",
      "Epoch 25/50 | Loss: 0.2820\n",
      "Epoch 26/50 | Loss: 0.2776\n",
      "Epoch 27/50 | Loss: 0.2690\n",
      "Epoch 28/50 | Loss: 0.2598\n",
      "Epoch 29/50 | Loss: 0.2546\n",
      "Epoch 30/50 | Loss: 0.2515\n",
      "Epoch 31/50 | Loss: 0.2458\n",
      "Epoch 32/50 | Loss: 0.2380\n",
      "Epoch 33/50 | Loss: 0.2329\n",
      "Epoch 34/50 | Loss: 0.2302\n",
      "Epoch 35/50 | Loss: 0.2250\n",
      "Epoch 36/50 | Loss: 0.2192\n",
      "Epoch 37/50 | Loss: 0.2162\n",
      "Epoch 38/50 | Loss: 0.2128\n",
      "Epoch 39/50 | Loss: 0.2082\n",
      "Epoch 40/50 | Loss: 0.2048\n",
      "Epoch 41/50 | Loss: 0.2024\n",
      "Epoch 42/50 | Loss: 0.1979\n",
      "Epoch 43/50 | Loss: 0.1968\n",
      "Epoch 44/50 | Loss: 0.1925\n",
      "Epoch 45/50 | Loss: 0.1919\n",
      "Epoch 46/50 | Loss: 0.1878\n",
      "Epoch 47/50 | Loss: 0.1871\n",
      "Epoch 48/50 | Loss: 0.1838\n",
      "Epoch 49/50 | Loss: 0.1831\n",
      "Epoch 50/50 | Loss: 0.1797\n"
     ]
    }
   ],
   "source": [
    "# 모델 학습\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "epochs = 50\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train() \n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(inputs)\n",
    "\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f'Epoch {epoch + 1}/{epochs} | Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d455ce8e",
   "metadata": {},
   "source": [
    "# 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "758ef2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "\n",
    "def sentence_preprocessing(test_input_sentence):\n",
    "    okt = Okt()\n",
    "\n",
    "    # 인풋 처리\n",
    "    test_input = re.sub(r\"[^가-힣0-9\\s]\", \"\", test_input_sentence)\n",
    "    test_inut = re.sub(r\"\\s+\", \" \", test_input_sentence).strip()\n",
    "    test_tokens = tokens = okt.morphs(test_input_sentence, stem=True, norm=True)\n",
    "    test_tokens = filter_tokens(tokens)\n",
    "    #print(test_tokens)\n",
    "  \n",
    "    sequenced_tokens = vocab.lookup_indices(test_tokens) # 시퀀싱\n",
    "    #print(sequenced_tokens)\n",
    "\n",
    "    # 패딩\n",
    "    padded_tokens = [pad(sequenced_tokens, 20, 0)] # 패딩\n",
    "    #print(padded_tokens)\n",
    "\n",
    "    return torch.tensor(padded_tokens, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e29d4d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "리뷰: 사장님이 미쳤나 싶을 정도입니다\n",
      "확률: 0.9522188305854797\n",
      "긍정\n",
      "\n",
      "리뷰: 고기가 너무 질겨요...\n",
      "확률: 0.17332422733306885\n",
      "부정\n",
      "\n",
      "리뷰: 이 집은 진짜 꼭 가야합니다\n",
      "확률: 0.9739217162132263\n",
      "긍정\n",
      "\n",
      "리뷰: 다시는 이 집에 가지 마세요\n",
      "확률: 0.05102381855249405\n",
      "부정\n",
      "\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    test_input_sentence = input(\"감정 분석을 할 문장을 입력해주세요: \")\n",
    "    \n",
    "    # 종료\n",
    "    if test_input_sentence == \"X\":\n",
    "        break\n",
    "    \n",
    "    print(\"리뷰:\", test_input_sentence)\n",
    "    test_input = sentence_preprocessing(test_input_sentence)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(test_input)\n",
    "        print(\"확률:\", float(output[0][0]))\n",
    "        if float(output[0][0]) > 0.5:\n",
    "            print(\"긍정\")\n",
    "        else:\n",
    "            print(\"부정\")\n",
    "    \n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
