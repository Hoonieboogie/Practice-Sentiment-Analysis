{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7eb76268",
   "metadata": {},
   "source": [
    "# [Practice] Sentiment Analysis Practice \n",
    "- 한국 식당 리뷰 감성분석\n",
    "- <커널 재시작시 여기서부터 시작> 부분으로 이동해서 실행"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7eaea23",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacf2797",
   "metadata": {},
   "source": [
    "### 데이터 로드"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb42465",
   "metadata": {},
   "source": [
    "데이터 출처: https://huggingface.co/datasets/leey4n/KR3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a98bbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "kr3 = load_dataset(\"leey4n/KR3\", split='train')\n",
    "kr3 = kr3.remove_columns(['__index_level_0__']) # Original file didn't include this column. Suspect it's a hugging face issue.\n",
    "\n",
    "# 레이블이 2(모호한 답변)인 리뷰 drop\n",
    "kr3_binary = kr3.filter(lambda example: example['Rating'] != 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c441ce53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rating</th>\n",
       "      <th>Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>숙성 돼지고기 전문점입니다. 건물 모양 때문에 매장 모양도 좀 특이하지만 쾌적한 편...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>고기가 정말 맛있었어요! 육즙이 가득 있어서 너무 좋았아요 일하시는 분들 너무 친절...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>잡내 없고 깔끔, 담백한 맛의 순댓국이 순댓국을 안 좋아하는 사람들에게도 술술 넘어...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>고기 양이 푸짐해서 특 순대국밥을 시킨 기분이 듭니다 맛도 좋습니다 다만 양념장이 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>순댓국 자체는 제가 먹어본 순대국밥집 중에서 Top5 안에는 들어요. 그러나 밥 양...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459016</th>\n",
       "      <td>0</td>\n",
       "      <td>731 배달 시켜 먹었고요, 거리상 1.8km입니다. 배민에서 시켰고 정확히 58만...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459017</th>\n",
       "      <td>1</td>\n",
       "      <td>송탄 미군부대 근처에 위치한 곳 원래 로컬 맛 집으로 되게 유명했는데 삼대 천왕에 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459018</th>\n",
       "      <td>1</td>\n",
       "      <td>집에서 40킬로 정도 떨어져 있는 곳인데도 몇 달에 한 번은 이거 먹으러 일부러 갑...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459019</th>\n",
       "      <td>0</td>\n",
       "      <td>원래 글 안 쓰는데 이거는 정말 다른 분들 위해서 써야 할 것 같네요 방금 포장 주...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459020</th>\n",
       "      <td>1</td>\n",
       "      <td>우리 팀 단골집, 술 먹고 다음 날 가면 푸짐하게 배불리 해장할 수 있는 곳, 주말...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>459021 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Rating                                             Review\n",
       "0            1  숙성 돼지고기 전문점입니다. 건물 모양 때문에 매장 모양도 좀 특이하지만 쾌적한 편...\n",
       "1            1  고기가 정말 맛있었어요! 육즙이 가득 있어서 너무 좋았아요 일하시는 분들 너무 친절...\n",
       "2            1  잡내 없고 깔끔, 담백한 맛의 순댓국이 순댓국을 안 좋아하는 사람들에게도 술술 넘어...\n",
       "3            1  고기 양이 푸짐해서 특 순대국밥을 시킨 기분이 듭니다 맛도 좋습니다 다만 양념장이 ...\n",
       "4            1  순댓국 자체는 제가 먹어본 순대국밥집 중에서 Top5 안에는 들어요. 그러나 밥 양...\n",
       "...        ...                                                ...\n",
       "459016       0  731 배달 시켜 먹었고요, 거리상 1.8km입니다. 배민에서 시켰고 정확히 58만...\n",
       "459017       1  송탄 미군부대 근처에 위치한 곳 원래 로컬 맛 집으로 되게 유명했는데 삼대 천왕에 ...\n",
       "459018       1  집에서 40킬로 정도 떨어져 있는 곳인데도 몇 달에 한 번은 이거 먹으러 일부러 갑...\n",
       "459019       0  원래 글 안 쓰는데 이거는 정말 다른 분들 위해서 써야 할 것 같네요 방금 포장 주...\n",
       "459020       1  우리 팀 단골집, 술 먹고 다음 날 가면 푸짐하게 배불리 해장할 수 있는 곳, 주말...\n",
       "\n",
       "[459021 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 'reviews' 코퍼스와 'ratings' 라벨 저장\n",
    "reviews = kr3_binary[\"Review\"]\n",
    "ratings = kr3_binary['Rating']\n",
    "\n",
    "# 샘플 확인\n",
    "review_df = pd.DataFrame(kr3_binary, columns=kr3_binary.column_names)\n",
    "display(review_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52dce9e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'잡내 없고 깔끔, 담백한 맛의 순댓국이 순댓국을 안 좋아하는 사람들에게도 술술 넘어갈 듯합니다. 정식 메뉴를 시키면 구성비 좋게 찹쌀순대와 수육이 나오는데 아주 푸짐해요. 해장하러 갔는데 국물이 고소해서 소주를 더 시키게 되는... 여하튼 순댓국이 일품입니다. 송파에선 알아주는 터줏대감인듯해요'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_df['Review'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d49ff567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "긍정 리뷰 비율: 0.8455190503266735 | 부정 리뷰 비율: 0.15448094967332648\n"
     ]
    }
   ],
   "source": [
    "# 긍/부정 비율 확인 (긍정이 압도적으로 많음)\n",
    "positive_rate = len(review_df[review_df['Rating'] == 1]) / len(review_df)\n",
    "negative_rate = 1 - positive_rate\n",
    "print(\"긍정 리뷰 비율:\", positive_rate, \"|\", \"부정 리뷰 비율:\", negative_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1e47a2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a15e95",
   "metadata": {},
   "source": [
    "### 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acf0065c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# 1) 안전 불용어(기능어 중심)\n",
    "safe_stopwords_ko = set(\"\"\"\n",
    "은 는 이 가 을 를 에 의 와 과 도 만 보다 에서 에게 께 부터 까지 마다 한테 처럼 대로 하고 으로 로 으로써 로써\n",
    "그리고 그러나 하지만 또는 그래서 그런데 혹은 따라서 그러므로 고로\n",
    "저 나 너 우리 저희 너희 그 이 저기 거기 여기 무엇 무슨 누구 어디 어느 어떤 이것 그것 저것 이런 그런 저런 이번 이쪽 저쪽 그쪽 이때 그때\n",
    "다 요 죠 네 데 게 지 군 나 니다 합니다 했다 한다 된다 였다\n",
    "때 시간 동안 시각 무렵 이상 이하 전후 이래 이후\n",
    "또 또한 그냥 다시 계속 항상 여전히 아직 가끔 자주 거의\n",
    "아 어 야 응 음 흠 헐 우와 와 오 이야 휴 허 하하 ㅋㅋ ㅎㅎ ㅠㅠ\n",
    "\"\"\".split())\n",
    "\n",
    "# 2) 보호 단어(감성 핵심)\n",
    "protect_sentiment_words = set(\"\"\"\n",
    "별로 좋아 좋다 최고 최악 실망 만족 불만 추천 재방문\n",
    "맛있다 맛없다 싱겁다 짜다 달다 매콤 맵다 느끼하다 담백하다 신선 싱싱 비린 눅눅 바삭 촉촉 부드럽다 질기다\n",
    "양 많다 적다 푸짐 부족 비싸다 싸다 가성비 가심비\n",
    "빠르다 느리다 한산 복잡 붐빈다 대기 기다리다\n",
    "깨끗 청결 더럽 위생 지저분 냄새 쾌적 시끄럽다 조용하다\n",
    "서비스 응대 친절 불친절 상냥 무례 거칠다\n",
    "정말 진짜 너무 매우 아주 꽤 완전\n",
    "안 못 없다 없음 아니라 비추 다시는 다신\n",
    "\"\"\".split())\n",
    "\n",
    "# 3) 1글자 화이트리스트(도메인 핵심)\n",
    "whitelist_1char = set(list(\"맛밥국탕면롤육쌈김술밥빵밥밥양밥밥\")) | {\"맛\",\"밥\",\"국\",\"양\",\"술\",\"김\"}\n",
    "\n",
    "neg_prefix = {\"안\",\"못\",\"안함\",\"않\",\"못함\",\"무\",\"미\",\"불\",\"무척\"}  # 필요시 조정\n",
    "emphasis_words = {\"정말\",\"진짜\",\"너무\",\"매우\",\"아주\",\"완전\",\"꽤\"}\n",
    "\n",
    "def normalize_token(t: str) -> str:\n",
    "    # 숫자 통일 (선택)\n",
    "    if re.fullmatch(r\"\\d+\", t):\n",
    "        return \"NUM\"\n",
    "    # ㅋㅋㅋ, ㅎㅎㅎ 축약 (선택)\n",
    "    t = re.sub(r\"(ㅋ)\\1+\", r\"\\1\\1\", t)\n",
    "    t = re.sub(r\"(ㅎ)\\1+\", r\"\\1\\1\", t)\n",
    "    return t\n",
    "\n",
    "def join_negation(tokens):\n",
    "    \"\"\"'안/못/없다' + (다음 감성 단어) -> 결합 토큰 생성\"\"\"\n",
    "    joined = []\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        t = tokens[i]\n",
    "        if t in {\"안\",\"못\",\"없다\",\"없음\",\"아니다\",\"별로\"} and i+1 < len(tokens):\n",
    "            nxt = tokens[i+1]\n",
    "            # 결합 토큰 만들기\n",
    "            joined.append(f\"{t}_{nxt}\")\n",
    "            i += 2\n",
    "            continue\n",
    "        joined.append(t)\n",
    "        i += 1\n",
    "    return joined\n",
    "\n",
    "def filter_tokens(tokens,\n",
    "                  stopwords=safe_stopwords_ko,\n",
    "                  protect=protect_sentiment_words,\n",
    "                  min_len=1):\n",
    "    out = []\n",
    "    tokens = [normalize_token(t) for t in tokens]\n",
    "    tokens = join_negation(tokens)\n",
    "\n",
    "    for t in tokens:\n",
    "        if t in protect or t in emphasis_words:\n",
    "            out.append(t)                         # 보호\n",
    "            continue\n",
    "        if len(t) < min_len and t not in whitelist_1char:\n",
    "            continue                              # 1글자 필터 (화이트리스트 제외)\n",
    "        if t in stopwords:\n",
    "            continue                              # 불용어 제거\n",
    "        out.append(t)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50197f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 459021/459021 [2:22:51<00:00, 53.55it/s]   \n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from tqdm import tqdm\n",
    "\n",
    "okt = Okt()\n",
    "\n",
    "preprocessed_sentences = []\n",
    "\n",
    "for review in tqdm(reviews):\n",
    "    review = re.sub(r\"[^가-힣0-9\\s]\", \"\", review)   # 구두점, 특수문자, 영문 제거\n",
    "    review = re.sub(r\"\\s+\", \" \", review).strip()   # 여러 공백을 하나의 공백으로\n",
    "    tokens = okt.morphs(review, stem=True, norm=True) # 토큰화\n",
    "    tokens = filter_tokens(tokens)\n",
    "    preprocessed_sentences.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0b399c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['숙성',\n",
       "  '돼지고기',\n",
       "  '전문점',\n",
       "  '이다',\n",
       "  '건물',\n",
       "  '모양',\n",
       "  '때문',\n",
       "  '매장',\n",
       "  '모양',\n",
       "  '좀',\n",
       "  '특이하다',\n",
       "  '쾌적하다',\n",
       "  '편이',\n",
       "  '고',\n",
       "  '살짝',\n",
       "  '레트로',\n",
       "  '감성',\n",
       "  '분위기',\n",
       "  '잡다',\n",
       "  '모든',\n",
       "  '직원',\n",
       "  '분들',\n",
       "  '께서',\n",
       "  '전부',\n",
       "  '가능하다',\n",
       "  '멘트',\n",
       "  '치다',\n",
       "  '고기',\n",
       "  '초반',\n",
       "  '커팅',\n",
       "  '까지는',\n",
       "  '굽다',\n",
       "  '가격',\n",
       "  '저렴하다',\n",
       "  '편',\n",
       "  '아니다_맛',\n",
       "  '준수',\n",
       "  '하다',\n",
       "  '등심',\n",
       "  '덧',\n",
       "  '살이',\n",
       "  '인상',\n",
       "  '깊다',\n",
       "  '구이',\n",
       "  '별로_일',\n",
       "  '줄',\n",
       "  '알다',\n",
       "  '육',\n",
       "  '향',\n",
       "  '짙다',\n",
       "  '얇다',\n",
       "  '며',\n",
       "  '뻑뻑',\n",
       "  '하다',\n",
       "  '않다',\n",
       "  '하이라이트',\n",
       "  '된장찌개',\n",
       "  '진짜',\n",
       "  '굿',\n",
       "  '이다',\n",
       "  '버터',\n",
       "  '간장',\n",
       "  '밥',\n",
       "  '골뱅이',\n",
       "  '국수',\n",
       "  '등',\n",
       "  '나중',\n",
       "  '더',\n",
       "  '맛보다',\n",
       "  '하다',\n",
       "  '것',\n",
       "  '들',\n",
       "  '남기다',\n",
       "  '두다'],\n",
       " ['고기',\n",
       "  '정말',\n",
       "  '맛있다',\n",
       "  '육즙',\n",
       "  '가득',\n",
       "  '있다',\n",
       "  '너무',\n",
       "  '좋다',\n",
       "  '일',\n",
       "  '하다',\n",
       "  '분들',\n",
       "  '너무',\n",
       "  '친절하다',\n",
       "  '좋다',\n",
       "  '가격',\n",
       "  '조금',\n",
       "  '있다',\n",
       "  '그만하다',\n",
       "  '맛',\n",
       "  '이라고',\n",
       "  '생각']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_sentences[:2]    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52345ff",
   "metadata": {},
   "source": [
    "preprocessed_sentences 저장 & 로드 \n",
    "- 커널 재시작시 시간 줄이기용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53d5a1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, gzip\n",
    "\n",
    "# 저장 (리뷰마다 한 줄)\n",
    "with gzip.open(\"preprocessed_sentences.jsonl.gz\", \"wt\", encoding=\"utf-8\") as f:\n",
    "    for toks in preprocessed_sentences:\n",
    "        json.dump({\"tokens\": toks}, f, ensure_ascii=False)\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15efa1c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d52ce4",
   "metadata": {},
   "source": [
    "# 커널 재시작시 여기서부터 시작"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27393262",
   "metadata": {},
   "source": [
    "토큰화 및 전처리 완료된 corpus 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "025447b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['숙성', '돼지고기', '전문점', '이다', '건물', '모양', '때문', '매장', '모양', '좀', '특이하다', '쾌적하다', '편이', '고', '살짝', '레트로', '감성', '분위기', '잡다', '모든', '직원', '분들', '께서', '전부', '가능하다', '멘트', '치다', '고기', '초반', '커팅', '까지는', '굽다', '가격', '저렴하다', '편', '아니다_맛', '준수', '하다', '등심', '덧', '살이', '인상', '깊다', '구이', '별로_일', '줄', '알다', '육', '향', '짙다', '얇다', '며', '뻑뻑', '하다', '않다', '하이라이트', '된장찌개', '진짜', '굿', '이다', '버터', '간장', '밥', '골뱅이', '국수', '등', '나중', '더', '맛보다', '하다', '것', '들', '남기다', '두다'], ['고기', '정말', '맛있다', '육즙', '가득', '있다', '너무', '좋다', '일', '하다', '분들', '너무', '친절하다', '좋다', '가격', '조금', '있다', '그만하다', '맛', '이라고', '생각']]\n"
     ]
    }
   ],
   "source": [
    "# preprocessed_sentences 로드\n",
    "preprocessed_sentences = []\n",
    "with gzip.open(\"preprocessed_sentences.jsonl.gz\", \"rt\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        preprocessed_sentences.append(json.loads(line)[\"tokens\"])\n",
    "\n",
    "print(preprocessed_sentences[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8ce72b",
   "metadata": {},
   "source": [
    "단어사전 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6847dbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import json\n",
    "\n",
    "SPECIALS = [\"<pad>\", \"<unk>\"]  # 0: pad, 1: unk\n",
    "\n",
    "class SimpleVocab:\n",
    "    def __init__(self, itos, unk_token=\"<unk>\"):\n",
    "        self._itos = list(itos)                       # index -> token\n",
    "        self._stoi = {t:i for i,t in enumerate(itos)} # token -> index\n",
    "        self.unk = unk_token\n",
    "        self.unk_id = self._stoi.get(unk_token, 1)\n",
    "\n",
    "    def __len__(self): return len(self._itos)\n",
    "    def lookup_indices(self, tokens): return [self._stoi.get(t, self.unk_id) for t in tokens]\n",
    "    def lookup_token(self, idx): return self._itos[idx]\n",
    "    def get_stoi(self): return self._stoi\n",
    "    def get_itos(self): return self._itos\n",
    "\n",
    "    def save(self, path):\n",
    "        with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump({\"itos\": self._itos, \"unk\": self.unk}, f, ensure_ascii=False)\n",
    "\n",
    "    @staticmethod\n",
    "    def load(path):\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            obj = json.load(f)\n",
    "        return SimpleVocab(obj[\"itos\"], unk_token=obj.get(\"unk\",\"<unk>\"))\n",
    "\n",
    "def build_vocab_simple(tokenized_sentences, min_freq=2, max_size=None, specials=SPECIALS):\n",
    "    \"\"\"\n",
    "    min_freq: 최소 등장 빈도 (희귀어를 <unk>로 처리)\n",
    "    max_size: 특수토큰 포함한 전체 vocab 최대 크기 (None이면 제한 없음)\n",
    "    정렬: 빈도 descending, 동률이면 사전순\n",
    "    \"\"\"\n",
    "    counter = Counter()\n",
    "    for sent in tokenized_sentences:\n",
    "        counter.update(sent)\n",
    "\n",
    "    # 빈도 필터\n",
    "    items = [(w, c) for w, c in counter.items() if c >= min_freq]\n",
    "    # 빈도 내림차순, 동률 사전순\n",
    "    items.sort(key=lambda x: (-x[1], x[0]))\n",
    "\n",
    "    # 특수토큰 먼저, 그 뒤 단어들\n",
    "    words = [w for w, _ in items]\n",
    "    itos = list(specials) + words\n",
    "\n",
    "    # max_size 제한 (특수토큰 포함하여 자르기)\n",
    "    if max_size is not None and len(itos) > max_size:\n",
    "        itos = itos[:max_size]\n",
    "\n",
    "    return SimpleVocab(itos, unk_token=\"<unk>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "168a4f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000 [929, 604, 701, 9, 592, 1177, 160, 244, 1177, 38, 492, 1306, 217, 41, 190, 3565, 1172, 43, 388, 364] ['숙성', '돼지고기', '전문점', '이다', '건물', '모양', '때문', '매장', '모양', '좀']\n"
     ]
    }
   ],
   "source": [
    "vocab = build_vocab_simple(preprocessed_sentences, min_freq=2, max_size=30000)\n",
    "ids = vocab.lookup_indices(preprocessed_sentences[0])\n",
    "print(len(vocab), ids[:20], [vocab.lookup_token(i) for i in ids[:10]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03382f28",
   "metadata": {},
   "source": [
    "vocab 만든거 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd93809e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.save(\"vocab.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59b66d0",
   "metadata": {},
   "source": [
    "vocab 다시 로드해서 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0bb57b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = SimpleVocab.load(\"vocab.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23071341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시퀀싱\n",
    "sequenced_tokens = [vocab.lookup_indices(tokens) for tokens in preprocessed_sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d41e978",
   "metadata": {},
   "source": [
    "패딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1efa42e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최소 길이: 0\n",
      "최대 길이: 1169\n",
      "평균 길이: 38.26029745915764\n",
      "중앙값: 21.0\n",
      "95% 분위수: 130.0\n",
      "99% 분위수: 256.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "lengths = [len(seq) for seq in sequenced_tokens]\n",
    "\n",
    "print(\"최소 길이:\", np.min(lengths))\n",
    "print(\"최대 길이:\", np.max(lengths))\n",
    "print(\"평균 길이:\", np.mean(lengths))\n",
    "print(\"중앙값:\", np.median(lengths))\n",
    "print(\"95% 분위수:\", np.percentile(lengths, 95))\n",
    "print(\"99% 분위수:\", np.percentile(lengths, 99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d28cbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = list(kr3_binary[\"Rating\"])   # 정답 라벨 (0/1)\n",
    "\n",
    "# 입력-라벨 동기화 (빈 시퀀스 제거)\n",
    "filtered_pairs = [(seq, y) for seq, y in zip(sequenced_tokens, ratings) if len(seq) > 0]\n",
    "\n",
    "sequenced_tokens, labels = map(list, zip(*filtered_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "efcc79e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "455822 455822\n",
      "[929, 604, 701, 9, 592, 1177, 160, 244, 1177, 38, 492, 1306, 217, 41, 190, 3565, 1172, 43, 388, 364, 82, 161, 688, 828, 288, 4788, 697, 39, 2623, 4838, 1580, 183, 26, 231, 211, 2413, 3284, 2, 641, 8094, 711, 422, 476, 298, 4714, 116, 150, 707, 93, 4068, 495, 1055, 2750, 2, 13, 4736, 801, 37, 484, 9, 340, 466, 54, 3508, 367, 173, 783, 29, 269, 2, 10, 14, 637, 306]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(len(sequenced_tokens), len(labels))\n",
    "print(sequenced_tokens[0])  \n",
    "print(labels[0])         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "31fc069f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(ids, max_len, pad_id=0):\n",
    "    return ids[:max_len] + [pad_id] * max(0, max_len - len(ids))\n",
    "\n",
    "max_len = 130  # 95% 분위\n",
    "pad_id = vocab.get_stoi()[\"<pad>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4d6826cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_sequences = [pad(seq, max_len, pad_id) for seq in sequenced_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "abf520b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[929, 604, 701, 9, 592, 1177, 160, 244, 1177, 38, 492, 1306, 217, 41, 190, 3565, 1172, 43, 388, 364, 82, 161, 688, 828, 288, 4788, 697, 39, 2623, 4838, 1580, 183, 26, 231, 211, 2413, 3284, 2, 641, 8094, 711, 422, 476, 298, 4714, 116, 150, 707, 93, 4068, 495, 1055, 2750, 2, 13, 4736, 801, 37, 484, 9, 340, 466, 54, 3508, 367, 173, 783, 29, 269, 2, 10, 14, 637, 306, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "130\n",
      "pad id: 0\n"
     ]
    }
   ],
   "source": [
    "print(padded_sequences[0])\n",
    "print(len(padded_sequences[0]))     # max_len = 130 확인\n",
    "print(\"pad id:\", pad_id)        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53551634",
   "metadata": {},
   "source": [
    "Torch Tensor 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d371254a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "inputs = torch.tensor(padded_sequences, dtype=torch.long)           # shape: (N, T=130)\n",
    "labels = torch.tensor(labels, dtype=torch.float32).unsqueeze(1)     # shape: [N, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a90004",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e9ba7d",
   "metadata": {},
   "source": [
    "### 모델 생성 및 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8ac324",
   "metadata": {},
   "source": [
    "모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2806bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5d939a4a",
   "metadata": {},
   "source": [
    "모델 인스턴스 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a56ee59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "362ee8ab",
   "metadata": {},
   "source": [
    "모델 학습"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
